{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1369c1",
   "metadata": {},
   "source": [
    "# Web Scraping Internshala website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317c2ed",
   "metadata": {},
   "source": [
    "### Importing all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d98754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "from html.parser import HTMLParser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a59bc",
   "metadata": {},
   "source": [
    "### Parsing through website and scraping all the data from website and then appending the data to corresponding lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd63ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_title =[]   \n",
    "list_of_company=[]\n",
    "list_of_location=[]\n",
    "list_of_duration=[]\n",
    "list_of_stipend=[]\n",
    "list_of_hiring_since=[]\n",
    "list_of_oppor_posted=[]\n",
    "list_of_perks=[]\n",
    "list_of_skills=[]\n",
    "list_of_openings=[]\n",
    "\n",
    "\n",
    "for page_number in range(1,101):\n",
    "    \n",
    "    header = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36','Accept-Language':'en-US ,en;q=0.5'})\n",
    "    url = f\"https://internshala.com/internships/page-{page_number}\"\n",
    "    page = requests.get(url,headers=header)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    division = soup.find('div',attrs= {'class':'internship_list_container','id':'internship_list_container'})\n",
    "    sub_division = division.find('div',attrs= {'id':f'internship_list_container_{page_number}'})\n",
    "    full_card = sub_division.find_all('div',attrs= {'class':'container-fluid individual_internship visibilityTrackerItem'})\n",
    "\n",
    "    button_list = []\n",
    "    def full_button():\n",
    "        for i in full_card:\n",
    "            button_container =  i.find('div',attrs={'class':'button_container'})\n",
    "            button_list.append(button_container)\n",
    "        return  button_list\n",
    "\n",
    "    job_link_list=[]\n",
    "    def card_links_single_page():\n",
    "        for f in full_button():\n",
    "            view_detail_link = f.find('a',attrs={'class':'view_detail_button'})\n",
    "            link = view_detail_link.get('href')\n",
    "            job_link = 'https://internshala.com/'+link\n",
    "            job_link_list.append(job_link)\n",
    "        return job_link_list\n",
    "\n",
    "\n",
    "    for card_link in card_links_single_page():\n",
    "\n",
    "        header = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36','Accept-Language':'en-US ,en;q=0.5'})\n",
    "        card_url = card_link\n",
    "        page = requests.get(card_url,headers=header)\n",
    "        soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "        def title_ex():\n",
    "            try:\n",
    "                title = soup.find('span',attrs={'class','profile_on_detail_page'}).text.strip()\n",
    "            except:\n",
    "                title=\"\"\n",
    "            return title\n",
    "\n",
    "        def company_ex():\n",
    "            try:\n",
    "                company = soup.find('a',attrs={'class','link_display_like_text view_detail_button'}).text.strip()\n",
    "            except:\n",
    "                company =\"\"\n",
    "            return company\n",
    "\n",
    "        def location_ex():\n",
    "            try:\n",
    "                location = soup.find('a',attrs={'class','location_link view_detail_button'}).text.strip()\n",
    "            except:\n",
    "                location=\"\"\n",
    "            return location\n",
    "\n",
    "        def duration_ex():\n",
    "            try:\n",
    "                details_container = soup.find_all('div',attrs={'class','internship_other_details_container'})\n",
    "                for i in details_container:\n",
    "                    item_duration =i.find_all('div',attrs={'class','item_body'})\n",
    "\n",
    "                duration = ''.join(item_duration[1].text.strip())\n",
    "            except:\n",
    "                duration =\"\"\n",
    "            return duration\n",
    "\n",
    "        def stipend_ex():\n",
    "            try:\n",
    "                stipend = soup.find('span',attrs={'class','stipend'}).text\n",
    "            except:\n",
    "                stipend=\"\"\n",
    "            return stipend\n",
    "\n",
    "        def hiring_since_ex():\n",
    "            try:\n",
    "                activity_container = soup.find_all('div',attrs={'class':'activity_container'})\n",
    "                for j in activity_container:\n",
    "                    item_activity_container = j.find_all('div',attrs={'class':'activity'})\n",
    "                hiring_since= ''.join(item_activity_container[0].text.strip())\n",
    "            except:\n",
    "                hiring_since=\"\"\n",
    "            return hiring_since \n",
    "\n",
    "        def oppor_posted_ex():\n",
    "            try:\n",
    "                activity_container = soup.find_all('div',attrs={'class':'activity_container'})\n",
    "                for j in activity_container:\n",
    "                    item_activity_container = j.find_all('div',attrs={'class':'activity'})\n",
    "                oppor_posted = ''.join(item_activity_container[1].text.strip())\n",
    "            except:\n",
    "                oppor_posted = \"\"\n",
    "            return oppor_posted\n",
    "\n",
    "        def perks_ex():\n",
    "            try:\n",
    "                aaa = soup.find_all('div',{'class':'section_heading heading_5_5'})\n",
    "                empty_perks=[]\n",
    "                def perk_find():\n",
    "                    def perk():\n",
    "                        for p in aaa:\n",
    "                            if p.text=='Perks':\n",
    "                                return p\n",
    "                    if perk() != None:\n",
    "                        bbb= perk().find_next('div')\n",
    "                        ccc = bbb.find_all('span')\n",
    "                        perks_duplicate=[]\n",
    "                        for i in ccc:\n",
    "                            perks_duplicate.append(i.text.strip())\n",
    "                        return perks_duplicate\n",
    "                    else:\n",
    "                        return empty_perks\n",
    "                perks = ','.join(perk_find())\n",
    "            except:\n",
    "                perks = \"\"\n",
    "            return perks\n",
    "\n",
    "        def skills_ex():\n",
    "            try:\n",
    "                aaa = soup.find_all('div',{'class':'section_heading heading_5_5'})\n",
    "                empty_skills = []     \n",
    "                def skill_find():\n",
    "                    def skill():\n",
    "                        for s in aaa:\n",
    "                            if s.text=='Skill(s) required':\n",
    "                                return s\n",
    "\n",
    "                    if skill() != None:\n",
    "                        ddd = skill().find_next('div')\n",
    "                        eee = ddd.find_all('span')\n",
    "                        skills_duplicate=[]\n",
    "                        for i in eee:\n",
    "                            skills_duplicate.append(i.text.strip())\n",
    "                        return skills_duplicate\n",
    "                    else:\n",
    "                        return empty_skills\n",
    "                skills= ','.join(skill_find())  \n",
    "            except:\n",
    "                skills=\"\"\n",
    "            return skills\n",
    "\n",
    "        def openings_ex():\n",
    "            try:\n",
    "                aaa = soup.find_all('div',{'class':'section_heading heading_5_5'})\n",
    "                def opening():\n",
    "                    for o in aaa:\n",
    "                        if o.text=='Number of openings':\n",
    "                            return o\n",
    "\n",
    "                fff = opening().find_next('div')\n",
    "                openings = fff.text.strip()\n",
    "            except:\n",
    "                openings=\"\"\n",
    "            return openings\n",
    "\n",
    "        list_of_title.append(title_ex())\n",
    "        list_of_company.append(company_ex())\n",
    "        list_of_location.append(location_ex())\n",
    "        list_of_duration.append(duration_ex())\n",
    "        list_of_stipend.append(stipend_ex())\n",
    "        list_of_hiring_since.append(hiring_since_ex())\n",
    "        list_of_oppor_posted.append(oppor_posted_ex())\n",
    "        list_of_perks.append(perks_ex())\n",
    "        list_of_skills.append(skills_ex())\n",
    "        list_of_openings.append(openings_ex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e58dde",
   "metadata": {},
   "source": [
    "### Converting the appended lists into Dataframe with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'title':list_of_title,\n",
    "              'company':list_of_company,\n",
    "              'location':list_of_location,\n",
    "              'duration':list_of_duration ,\n",
    "              'stipend':list_of_stipend,\n",
    "              'hiring_since':list_of_hiring_since,\n",
    "              'oppor_posted':list_of_oppor_posted,\n",
    "              'perks':list_of_perks,\n",
    "              'skills':list_of_skills,\n",
    "              'openings':list_of_openings}\n",
    "df = pd.DataFrame.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335808b1",
   "metadata": {},
   "source": [
    "### Exporting the Dataframe to external file (CSV file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:\\\\Users\\\\kumar\\\\Desktop\\\\internshala_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8bc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb32c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
